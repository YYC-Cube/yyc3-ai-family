# iMac M4 Ollama ä¼˜åŒ–å¿«é€Ÿå‚è€ƒ

> å¿«é€Ÿå‘½ä»¤å’Œé…ç½®å‚è€ƒ

---

## ğŸš€ å¿«é€Ÿå¯åŠ¨

```bash
# æ–¹å¼ 1: ä½¿ç”¨ä¼˜åŒ–å¯åŠ¨è„šæœ¬ï¼ˆæ¨èï¼‰
bash /Users/yanyu/Family-Ï€Â³/scripts/ollama-optimized-start.sh

# æ–¹å¼ 2: æ‰‹åŠ¨å¯åŠ¨
OLLAMA_HOST=0.0.0.0 OLLAMA_MAX_LOADED_MODELS=1 OLLAMA_LOAD_TIMEOUT=10m0s OLLAMA_KEEP_ALIVE=10m0s ollama serve
```

---

## âš™ï¸ æ ¸å¿ƒç¯å¢ƒå˜é‡

```bash
OLLAMA_HOST=0.0.0.0:11434          # ç›‘å¬æ‰€æœ‰ç½‘ç»œæ¥å£
OLLAMA_MAX_LOADED_MODELS=1         # é™åˆ¶åŒæ—¶åŠ è½½çš„æ¨¡å‹æ•°
OLLAMA_LOAD_TIMEOUT=10m0s          # å¢åŠ åŠ è½½è¶…æ—¶
OLLAMA_KEEP_ALIVE=10m0s            # å»¶é•¿æ¨¡å‹å­˜æ´»æ—¶é—´
OLLAMA_NUM_PARALLEL=1              # å•çº¿ç¨‹åŠ è½½
OLLAMA_DEBUG=INFO                  # è°ƒè¯•çº§åˆ«
```

---

## ğŸ¤– æ¨èæ¨¡å‹

| ç”¨é€” | æ¨¡å‹ | å‘½ä»¤ |
|------|------|------|
| å¿«é€Ÿå“åº” | phi3:mini | `ollama pull phi3:mini` |
| ä»£ç ç”Ÿæˆ | codegeex4:9b | `ollama pull codegeex4:9b` |
| é€šç”¨ä»»åŠ¡ | qwen2.5:7b | `ollama pull qwen2.5:7b` |

---

## ğŸ” ç›‘æ§å‘½ä»¤

```bash
# æŸ¥çœ‹ GPU ä½¿ç”¨ç‡
sudo powermetrics --samplers gpu_power -i 1000

# æŸ¥çœ‹æ˜¾å­˜ä½¿ç”¨
curl -s http://localhost:11434/api/tags | jq '.models | map({name, size, running})'

# æŸ¥çœ‹å·²åŠ è½½æ¨¡å‹
curl -s http://localhost:11434/api/tags | jq '.models[] | select(.running == true)'

# æµ‹è¯•å“åº”æ—¶é—´
time curl -s http://localhost:11434/api/generate -X POST -H 'Content-Type: application/json' -d '{"model":"phi3:mini","prompt":"Hi","stream":false}'

# æŸ¥çœ‹æ—¥å¿—
tail -f /tmp/ollama-optimized.log
```

---

## ğŸ› ï¸ å¸¸ç”¨æ“ä½œ

```bash
# åˆ—å‡ºæ‰€æœ‰æ¨¡å‹
ollama list

# æ‹‰å–æ¨¡å‹
ollama pull <model>

# åˆ é™¤æ¨¡å‹
ollama rm <model>

# æ¸…ç†ç¼“å­˜
ollama gc

# å¸è½½å½“å‰æ¨¡å‹
curl -X POST http://localhost:11434/api/generate -d '{"model":"phi3:mini","keep_alive":0}'

# åœæ­¢æœåŠ¡
pkill -f "ollama serve"
```

---

## ğŸ“Š API è¯·æ±‚ç¤ºä¾‹

### åŸºç¡€è¯·æ±‚

```bash
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi3:mini",
    "prompt": "Hello, how are you?",
    "stream": false
  }'
```

### ä¼˜åŒ–ä¸Šä¸‹æ–‡

```bash
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi3:mini",
    "prompt": "Hello",
    "options": {
      "num_ctx": 8192,
      "num_batch": 512
    }
  }'
```

### æµå¼å“åº”

```bash
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi3:mini",
    "prompt": "Hello",
    "stream": true
  }'
```

---

## ğŸ¯ æ€§èƒ½ç›®æ ‡

| æŒ‡æ ‡ | ç›®æ ‡å€¼ |
|------|--------|
| æ¨¡å‹åŠ è½½æ—¶é—´ | < 5 ç§’ |
| æ˜¾å­˜ä½¿ç”¨ç‡ | < 50% |
| å“åº”æ—¶é—´ | < 3ms |
| å¹¶å‘æ¨¡å‹æ•° | 1 |

---

## ğŸ“ ç›¸å…³æ–‡ä»¶

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `ollama-optimized-start.sh` | ä¼˜åŒ–å¯åŠ¨è„šæœ¬ |
| `ollama-optimization.sh` | ä¼˜åŒ–å·¥å…·è„šæœ¬ |
| `ollama-optimization.env` | ç¯å¢ƒå˜é‡é…ç½® |
| `iMac-M4-Analysis.md` | è¯¦ç»†åˆ†ææŠ¥å‘Š |
| `iMac-M4-Optimization-Guide.md` | å®Œæ•´ä¼˜åŒ–æŒ‡å— |

---

## ğŸ”§ æ•…éšœæ’æŸ¥

### æœåŠ¡æ— æ³•å¯åŠ¨

```bash
# æ£€æŸ¥ç«¯å£å ç”¨
lsof -i :11434

# æŸ¥çœ‹æ—¥å¿—
tail -f /tmp/ollama-optimized.log
```

### æ¨¡å‹åŠ è½½è¶…æ—¶

```bash
# å¢åŠ è¶…æ—¶æ—¶é—´
export OLLAMA_LOAD_TIMEOUT=15m0s

# æ¸…ç†ç¼“å­˜
ollama gc
```

### æ˜¾å­˜ä¸è¶³

```bash
# å¸è½½æ¨¡å‹
curl -X POST http://localhost:11434/api/generate -d '{"model":"phi3:mini","keep_alive":0}'

# ä½¿ç”¨æ›´å°çš„ä¸Šä¸‹æ–‡
curl -X POST http://localhost:11434/api/generate -d '{"model":"phi3:mini","prompt":"Hi","options":{"num_ctx":4096}}'
```

### ç½‘ç»œè¿æ¥å¤±è´¥

```bash
# æ£€æŸ¥é˜²ç«å¢™
sudo pfctl -s rules | grep 11434

# æµ‹è¯•è¿æ¥
ping 192.168.3.22
telnet 192.168.3.22 11434
```

---

## ğŸ“ è·å–å¸®åŠ©

- æŸ¥çœ‹å®Œæ•´æŒ‡å—: `cat /Users/yanyu/Family-Ï€Â³/scripts/iMac-M4-Optimization-Guide.md`
- æŸ¥çœ‹åˆ†ææŠ¥å‘Š: `cat /Users/yanyu/Family-Ï€Â³/scripts/iMac-M4-Analysis.md`
- è¿è¡Œä¼˜åŒ–å·¥å…·: `bash /Users/yanyu/Family-Ï€Â³/scripts/ollama-optimization.sh`

---

<div align="center">

**YYCÂ³ AI Family**
*è¨€å¯è±¡é™ | è¯­æ¢æœªæ¥*

</div>
