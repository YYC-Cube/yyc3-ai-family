# ============================================================
# iMac M4 Ollama 连接优化配置
# 目标: 减少模型加载时间 + 优化显存使用
# ============================================================

# ============================================================
# 网络连接优化
# ============================================================

# 监听所有网络接口（必须）
OLLAMA_HOST=0.0.0.0:11434

# 允许跨域请求（已配置，保持）
OLLAMA_ORIGINS="http://localhost:3113,http://192.168.3.22:3113,http://localhost,https://localhost,http://localhost:*,https://localhost:*,http://127.0.0.1,https://127.0.0.1,http://127.0.0.1:*,https://127.0.0.1:*,http://0.0.0.0,https://0.0.0.0,http://0.0.0.0:*,https://0.0.0.0:*,app://*,file://*,tauri://*,vscode-webview://*,vscode-file://*"

# ============================================================
# 模型加载优化（减少加载时间）
# ============================================================

# 增加加载超时时间（从 5m 增加到 10m）
OLLAMA_LOAD_TIMEOUT=10m0s

# 限制同时加载的模型数（从 0 改为 1，避免显存溢出）
OLLAMA_MAX_LOADED_MODELS=1

# 延长模型存活时间（从 5m 增加到 10m，减少重新加载）
OLLAMA_KEEP_ALIVE=10m0s

# 单线程加载（避免资源竞争）
OLLAMA_NUM_PARALLEL=1

# ============================================================
# 显存优化
# ============================================================

# 禁用云同步（减少网络开销）
OLLAMA_NO_CLOUD=false

# 禁用历史记录（减少内存占用）
OLLAMA_NOHISTORY=false

# 启用多用户缓存（减少重复加载）
OLLAMA_MULTIUSER_CACHE=false

# ============================================================
# 性能优化
# ============================================================

# 调试级别（生产环境设为 INFO）
OLLAMA_DEBUG=INFO

# Flash Attention（自动检测）
OLLAMA_FLASH_ATTENTION=false

# KV 缓存类型（默认）
OLLAMA_KV_CACHE_TYPE=

# GPU 开销（默认）
OLLAMA_GPU_OVERHEAD=0

# 上下文长度（默认 0，使用模型默认值）
OLLAMA_CONTEXT_LENGTH=0

# ============================================================
# 使用方法
# ============================================================

# 方式 1: 直接使用环境变量启动
# source ollama-optimization.env && ollama serve

# 方式 2: 在启动时指定
# OLLAMA_HOST=0.0.0.0 OLLAMA_MAX_LOADED_MODELS=1 OLLAMA_LOAD_TIMEOUT=10m0s OLLAMA_KEEP_ALIVE=10m0s ollama serve

# 方式 3: 创建启动脚本
# #!/bin/bash
# export OLLAMA_HOST=0.0.0.0
# export OLLAMA_MAX_LOADED_MODELS=1
# export OLLAMA_LOAD_TIMEOUT=10m0s
# export OLLAMA_KEEP_ALIVE=10m0s
# ollama serve

# ============================================================
# 推荐模型配置
# ============================================================

# 小型模型（快速加载，低显存）
# - phi3:mini (3.8B, Q4_K_M, ~2.3 GB)
# - qwen2.5:3b (3B, Q4_K_M, ~1.9 GB)
# - gemma2:2b (2B, Q4_K_M, ~1.4 GB)

# 中型模型（平衡性能和显存）
# - phi3:medium (14B, Q4_K_M, ~8.5 GB)
# - qwen2.5:7b (7B, Q4_K_M, ~4.3 GB)
# - codegeex4:9b (9B, Q4_0, ~5.1 GB)

# 大型模型（高性能，高显存）
# - qwen2.5:14b (14B, Q4_K_M, ~8.5 GB)
# - llama3.1:8b (8B, Q4_K_M, ~4.7 GB)

# ============================================================
# API 请求优化示例
# ============================================================

# 1. 使用较小的上下文长度
# curl -X POST http://localhost:11434/api/generate \
#   -H "Content-Type: application/json" \
#   -d '{
#     "model": "phi3:mini",
#     "prompt": "Hello",
#     "options": {
#       "num_ctx": 8192,
#       "num_batch": 512
#     }
#   }'

# 2. 流式响应（减少延迟）
# curl -X POST http://localhost:11434/api/generate \
#   -H "Content-Type: application/json" \
#   -d '{
#     "model": "phi3:mini",
#     "prompt": "Hello",
#     "stream": true
#   }'

# 3. 卸载模型释放显存
# curl -X POST http://localhost:11434/api/generate \
#   -H "Content-Type: application/json" \
#   -d '{
#     "model": "phi3:mini",
#     "keep_alive": 0
#   }'

# ============================================================
# 监控命令
# ============================================================

# 1. 查看 GPU 使用率
# sudo powermetrics --samplers gpu_power -i 1000

# 2. 查看显存使用
# curl -s http://localhost:11434/api/tags | jq '.models | map({name, size, running})'

# 3. 查看已加载模型
# curl -s http://localhost:11434/api/tags | jq '.models[] | select(.running == true)'

# 4. 测试响应时间
# time curl -s http://localhost:11434/api/generate \
#   -X POST -H 'Content-Type: application/json' \
#   -d '{"model":"phi3:mini","prompt":"Hi","stream":false}'
