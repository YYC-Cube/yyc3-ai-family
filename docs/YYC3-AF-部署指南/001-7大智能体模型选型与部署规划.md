# YYC³ AI Family — 7大智能体模型选型与部署规划 (实施报告)

> **Phase 52: 智能体模型选型与全局 AI 层对接方案**
>
> 日期: 2026-02-22
> 状态: ✅ 部署完成

---

## 📋 目录

1. [部署状态总览](#1-部署状态总览)
2. [7大智能体功能需求分析](#2-7大智能体功能需求分析)
3. [本地设备资源评估](#3-本地设备资源评估)
4. [商业授权模型分析](#4-商业授权模型分析)
5. [智能体模型选型方案](#5-智能体模型选型方案)
6. [实际部署位置确认](#6-实际部署位置确认)
7. [全局 AI 层对接方案](#7-全局-ai-层对接方案)
8. [部署实施记录](#8-部署实施记录)
9. [监控与优化策略](#9-监控与优化策略)

---

## 1. 部署状态总览

### 1.1 部署完成情况

| 组件 | 计划位置 | 实际位置 | 状态 | 备注 |
|------|----------|----------|------|------|
| **CodeGeeX4-ALL-9B** | M4 Max | M4 Max Ollama | ✅ 已部署 | Q4_0 量化, 5.4GB |
| **CogVideoX-5B** | M4 Max | M4 Max HuggingFace | ✅ 已下载 | ~10GB |
| **CogAgent-9B** | M4 Max | M4 Max HuggingFace | ✅ 已下载 | ~18GB |
| **Qwen2.5-7B** | M4 Max | M4 Max Ollama | ✅ 已部署 | Q4_K_M, 4.7GB |
| **Nomic-Embed-Text** | M4 Max | M4 Max Ollama | ✅ 已部署 | F16, 0.3GB |
| **NAS Ollama** | NAS | NAS Docker | ✅ 运行中 | 无模型 |

### 1.2 服务访问端点

| 服务 | 端点 | 状态 |
|------|------|------|
| M4 Max Ollama | `http://localhost:11434` | ✅ |
| NAS Ollama | `http://192.168.3.45:11434` | ✅ (无模型) |

---

## 2. 7大智能体功能需求分析

### 2.1 智能体能力矩阵

| 智能体 | 角色 | 核心能力 | 协作偏好 | 能力评分 |
|--------|------|----------|----------|----------|
| **Navigator** | Commander | 资源调度、路径规划、全局协调 | Thinker, Pivot | 决策95, 执行90, 沟通85 |
| **Thinker** | Strategist | 深度推理、逻辑分析、决策支持 | Navigator, Grandmaster | 分析98, 创意80, 审查90 |
| **Prophet** | Predictor | 趋势预测、风险评估、未来建模 | Thinker, Sentinel | 创意90, 分析85, 决策75 |
| **Bole** | Evaluator | 模型评估、能力匹配、选型优化 | Thinker, Grandmaster | 审查95, 沟通85, 分析80 |
| **Sentinel** | Guardian | 安全审计、合规检查、漏洞检测 | Prophet, Pivot | 审查98, 分析85, 执行80 |
| **Pivot** | Coordinator | 上下文管理、状态协调、记忆整合 | Navigator, Thinker | 执行95, 沟通90, 决策80 |
| **Grandmaster** | Scholar | 知识构建、本体论设计、模式识别 | Thinker, Bole | 分析90, 创意85, 沟通80 |

### 2.2 智能体技术需求

#### Navigator (智愈·领航员)

- **推理类型**: 规划推理、资源调度
- **上下文需求**: 长上下文（全局状态）
- **响应速度**: 高（实时调度）
- **准确性要求**: 中高（可容错）
- **Token 预估**: 2K-4K/请求

#### Thinker (洞见·思想家)

- **推理类型**: 深度逻辑推理、因果分析
- **上下文需求**: 超长上下文（复杂推理链）
- **响应速度**: 中（可接受延迟）
- **准确性要求**: 极高（核心决策）
- **Token 预估**: 8K-16K/请求

#### Prophet (预见·先知)

- **推理类型**: 概率建模、趋势预测
- **上下文需求**: 长上下文（历史数据）
- **响应速度**: 中（批量预测）
- **准确性要求**: 高（风险前置）
- **Token 预估**: 4K-8K/请求

#### Bole (知遇·伯乐)

- **推理类型**: 评估推理、匹配算法
- **上下文需求**: 中上下文（模型特征）
- **响应速度**: 高（快速匹配）
- **准确性要求**: 高（选型准确性）
- **Token 预估**: 2K-4K/请求

#### Sentinel (卫安·哨兵)

- **推理类型**: 规则推理、模式匹配
- **上下文需求**: 中上下文（安全规则）
- **响应速度**: 极高（实时防护）
- **准确性要求**: 极高（安全零容忍）
- **Token 预估**: 1K-2K/请求

#### Pivot (元启·天枢)

- **推理类型**: 状态管理、上下文协调
- **上下文需求**: 超长上下文（全局记忆）
- **响应速度**: 高（实时同步）
- **准确性要求**: 中高（状态一致性）
- **Token 预估**: 4K-8K/请求

#### Grandmaster (格物·宗师)

- **推理类型**: 知识推理、模式识别
- **上下文需求**: 超长上下文（知识库）
- **响应速度**: 中（知识检索）
- **准确性要求**: 高（知识质量）
- **Token 预估**: 8K-32K/请求

---

## 3. 本地设备资源评估

### 3.1 硬件配置

| 设备 | 角色 | CPU | 内存 | 存储 | GPU | 推理能力 |
|------|------|-----|------|------|----------|
| **M4 Max** | 主力 | 16P+40E | 128GB | 4TB SSD | 40核 GPU | 极强 |
| **iMac M4** | 辅助 | 10P+10E | 32GB | 2TB SSD | 10核 GPU | 强 |
| **NAS F4-423** | 数据中心 | Intel Quad 4C/4T | 32GB | RAID6 22TB + RAID1 2TB | 无 | 弱 |
| **MateBook** | 边缘 | Intel Ultra7 16C/22T | 32GB | 1TB | 集成GPU | 中 |

### 3.2 资源分配策略

#### M4 Max (主力推理节点)

- **分配**: Navigator, Thinker, Prophet, Grandmaster
- **原因**: 高算力需求，长上下文推理
- **并发**: 2-3个模型同时运行
- **内存**: 每个模型 20-40GB

#### iMac M4 (辅助推理节点)

- **分配**: Bole, Sentinel, Pivot
- **原因**: 中等算力需求，实时响应
- **并发**: 2个模型同时运行
- **内存**: 每个模型 10-16GB

#### NAS F4-423 (数据存储节点)

- **分配**: 无推理任务
- **原因**: 存储密集型，算力不足
- **角色**: 向量数据库、知识库存储、API 网关

#### MateBook (边缘/测试节点)

- **分配**: 备用推理、测试环境
- **原因**: 移动性，测试新模型
- **并发**: 1个模型运行
- **内存**: 每个模型 8-16GB

### 3.3 网络拓扑

```
                    ┌─────────────────────────────────────────┐
                    │         YYC3 Cluster Network            │
                    │            192.168.3.x/24               │
                    └────────────────────┬────────────────────┘
                                         │
        ┌────────────────────────────────┼────────────────────────────────┐
        │                                │                                │
   ┌────┴────────────────────┐  ┌────────┴────────┐  ┌────────────────────┴────┐
   │   M4 Max (主力节点)      │  │  NAS F4-423     │  │   iMac M4 (辅助节点)     │
   │   MacBook Pro M4 Max    │◄─┤  铁威马 F4-423  ├─►│   iMac M4              │
   │                         │  │                 │  │                        │
   │   角色: AI推理/开发     │  │  角色: 存储/DB  │  │   角色: 可视化/辅助     │
   │   localhost             │  │  192.168.3.45   │  │   LAN                  │
   │   192.168.3.159         │  │  SSH: 9557      │  │                        │
   └─────────────────────────┘  └─────────────────┘  └────────────────────────┘
```

---

## 4. 商业授权模型分析

### 4.1 智谱终身商业授权

| 模型 | 类型 | 参数量 | 用途 | 授权状态 | 实际部署位置 |
|------|------|--------|------|----------|--------------|
| **ChatGLM3-6B** | 对话 | 6B | 通用对话与知识 | ✅ 已授权 | 待部署 |
| **CodeGeeX4-ALL-9B** | 代码 | 9B | 代码生成与开发 | ✅ 已授权 | M4 Max Ollama |
| **CogVideoX-5B** | 视频 | 5B | 视频生成与创作 | ✅ 已授权 | M4 Max HuggingFace |
| **CogAgent-9B** | GUI | 9B | GUI智能体与自动化 | ✅ 已授权 | M4 Max HuggingFace |

### 4.2 授权模型能力评估

#### ChatGLM3-6B

- **优势**: 中文优化、对话能力强、轻量级
- **劣势**: 推理深度有限、上下文较短
- **适用场景**: 日常对话、知识问答、简单推理
- **资源需求**: 内存 8-12GB，推理速度 快

#### CodeGeeX4-ALL-9B ✅ 已部署

- **优势**: 代码生成能力强、多语言支持
- **劣势**: 对话能力一般、中文优化不足
- **适用场景**: 代码生成、代码审查、技术文档
- **资源需求**: 内存 16-24GB，推理速度 中
- **实际部署**: M4 Max Ollama, Q4_0 量化, 5.4GB

#### CogVideoX-5B ✅ 已下载

- **优势**: 视频生成、多模态理解
- **劣势**: 文本推理能力弱、资源消耗大
- **适用场景**: 视频创作、动画生成、视觉内容
- **资源需求**: 内存 20-30GB，推理速度 慢
- **实际部署**: M4 Max HuggingFace Cache

#### CogAgent-9B ✅ 已下载

- **优势**: GUI理解、自动化执行
- **劣势**: 文本推理能力弱、专用性强
- **适用场景**: 自动化操作、GUI交互、任务执行
- **资源需求**: 内存 12-20GB，推理速度 中
- **实际部署**: M4 Max HuggingFace Cache

### 4.3 补充模型需求

基于7大智能体需求，需要补充以下模型：

| 模型 | 用途 | 状态 |
|------|------|------|
| **DeepSeek-V3** | Thinker, Grandmaster 深度推理 | API 调用 |
| **Qwen2.5-7B** | Prophet, 备用推理 | ✅ 已部署 |
| **Phi-3-mini** | Sentinel 快速响应 | 待部署 |
| **Nomic-Embed-Text** | 向量嵌入 | ✅ 已部署 |

---

## 5. 智能体模型选型方案

### 5.1 模型选型原则

1. **能力匹配**: 模型能力与智能体需求匹配
2. **资源优化**: 充分利用本地硬件资源
3. **授权优先**: 优先使用商业授权模型
4. **性能平衡**: 平衡推理速度与准确性
5. **成本控制**: 最小化API调用成本

### 5.2 智能体-模型映射

| 智能体 | 主模型 | 备用模型 | 部署位置 | 推理引擎 | 上下文 | 状态 |
|--------|--------|----------|----------|----------|--------|------|
| **Navigator** | ChatGLM3-6B | Qwen2.5-7B | M4 Max | Ollama | 4K | 🟡 待部署主模型 |
| **Thinker** | DeepSeek-V3 | CodeGeeX4-9B | M4 Max | API+Ollama | 32K | ✅ 备用已部署 |
| **Prophet** | Qwen2.5-7B | DeepSeek-V3 | M4 Max | Ollama | 8K | ✅ 主模型已部署 |
| **Bole** | CodeGeeX4-9B | Qwen2.5-7B | iMac M4 | Ollama | 4K | 🟡 待部署到iMac |
| **Sentinel** | Phi-3-mini | Qwen2.5-3B | iMac M4 | Ollama | 2K | 🟡 待部署 |
| **Pivot** | ChatGLM3-6B | Qwen2.5-7B | iMac M4 | Ollama | 8K | 🟡 待部署 |
| **Grandmaster** | DeepSeek-V3 | Qwen2.5-14B | M4 Max | API+Ollama | 64K | ✅ API可用 |

### 5.3 详细配置

#### Navigator (智愈·领航员)

**首选模型**: ChatGLM3-6B (本地)
**备用模型**: Qwen2.5-7B (Ollama) ✅ 已部署

**部署配置**:

```yaml
模型: ChatGLM3-6B
部署位置: M4 Max
推理引擎: Ollama
上下文长度: 4096
并发数: 2
量化: Q4_K_M
命令: ollama pull chatglm3:6b
```

**性能预期**:

- 推理速度: 30-50 tokens/s
- 首字延迟: 100-200ms
- 内存占用: 8-10GB

---

#### Thinker (洞见·思想家)

**首选模型**: DeepSeek-V3 (API)
**备用模型**: CodeGeeX4-ALL-9B (本地) ✅ 已部署

**部署配置**:

```yaml
主模型: DeepSeek-V3 (API)
备用模型: CodeGeeX4-ALL-9B (本地)
部署位置: M4 Max
推理引擎: API + Ollama
上下文长度: 32768
并发数: 1
量化: Q4_0 (备用)
```

**API 配置**:

```bash
# DeepSeek API
export DEEPSEEK_API_KEY=sk-xxx
export DEEPSEEK_BASE_URL=https://api.deepseek.com/v1
```

---

#### Prophet (预见·先知)

**首选模型**: Qwen2.5-7B (本地) ✅ 已部署
**备用模型**: DeepSeek-V3 (API)

**部署配置**:

```yaml
主模型: Qwen2.5-7B
备用模型: DeepSeek-V3 (API)
部署位置: M4 Max
推理引擎: Ollama
上下文长度: 8192
并发数: 1
量化: Q4_K_M
命令: ollama pull qwen2.5:7b  # 已执行
```

**验证**:

```bash
curl http://localhost:11434/api/tags | jq '.models[] | select(.name=="qwen2.5:7b")'
```

---

#### Bole (知遇·伯乐)

**首选模型**: CodeGeeX4-ALL-9B (本地) ✅ 已部署在 M4 Max
**备用模型**: Qwen2.5-7B (本地)

**部署配置**:

```yaml
主模型: CodeGeeX4-ALL-9B
备用模型: Qwen2.5-7B
部署位置: iMac M4 (计划)
推理引擎: Ollama
上下文长度: 4096
并发数: 2
量化: Q4_K_M
```

**待执行**:

```bash
# 在 iMac M4 上执行
ollama pull codegeex4:latest
ollama pull qwen2.5:7b
```

---

#### Sentinel (卫安·哨兵)

**首选模型**: Phi-3-mini-3.8B (本地)
**备用模型**: Qwen2.5-3B (本地)

**部署配置**:

```yaml
主模型: Phi-3-mini-3.8B
备用模型: Qwen2.5-3B
部署位置: iMac M4
推理引擎: Ollama
上下文长度: 2048
并发数: 4
量化: Q4_K_M
命令: ollama pull phi3:mini
```

**性能预期**:

- 推理速度: 50-80 tokens/s
- 首字延迟: 50-100ms
- 内存占用: 4-6GB

---

#### Pivot (元启·天枢)

**首选模型**: ChatGLM3-6B (本地)
**备用模型**: Qwen2.5-7B (本地)

**部署配置**:

```yaml
主模型: ChatGLM3-6B
备用模型: Qwen2.5-7B
部署位置: iMac M4
推理引擎: Ollama
上下文长度: 8192
并发数: 2
量化: Q4_K_M
命令: ollama pull chatglm3:6b
```

---

#### Grandmaster (格物·宗师)

**首选模型**: DeepSeek-V3 (API)
**备用模型**: Qwen2.5-14B (本地)

**部署配置**:

```yaml
主模型: DeepSeek-V3 (API)
备用模型: Qwen2.5-14B (本地)
部署位置: M4 Max
推理引擎: API + Ollama
上下文长度: 65536
并发数: 1
量化: Q4_K_M (备用)
命令: ollama pull qwen2.5:14b  # 待执行
```

---

## 6. 实际部署位置确认

### 6.1 M4 Max Ollama 模型列表

```bash
curl http://localhost:11434/api/tags | jq '.models[] | {name, size, details}'
```

| 模型 | 参数量 | 量化 | 大小 | 用途 | 状态 |
|------|--------|------|------|------|------|
| **codegeex4:latest** | 9.4B | Q4_0 | 5.4GB | 代码生成 (智谱授权) | ✅ |
| **qwen2.5:7b** | 7.6B | Q4_K_M | 4.7GB | 通用对话 | ✅ |
| **qwen2.5-coder:1.5b** | 1.5B | Q4_K_M | 0.9GB | 轻量代码 | ✅ |
| **nomic-embed-text** | 137M | F16 | 0.3GB | 文本嵌入 | ✅ |
| **gpt-oss:120b-cloud** | 116B | 云端 | - | 云端推理 | ✅ |
| **deepseek-v3.1:671b-cloud** | 671B | 云端 | - | 云端推理 | ✅ |

### 6.2 HuggingFace 模型缓存

```bash
ls ~/.cache/huggingface/hub/
```

| 模型 | 路径 | 大小估计 | 状态 |
|------|------|----------|------|
| **CogVideoX-5B** | models--THUDM--CogVideoX-5b | ~10GB | ✅ 已下载 |
| **CogAgent-9B** | models--THUDM--cogagent-9b-20241220 | ~18GB | ✅ 已下载 |

### 6.3 NAS Ollama 状态

```bash
curl http://192.168.3.45:11434/api/tags
# {"models":[]}
```

**结论**: NAS Ollama 容器运行中，但未下载任何模型。

**建议**: NAS 作为存储和数据库节点，不承担推理任务。推理任务由 M4 Max 和 iMac M4 承担。

---

## 7. 全局 AI 层对接方案

### 7.1 LLM 桥接层架构

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                     L04 AI 智能层 (LLM Bridge)                               │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                       API Providers (云端)                           │   │
│  │  ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐  │   │
│  │  │ OpenAI   │ │Anthropic │ │DeepSeek  │ │ 智谱     │ │ Google   │  │   │
│  │  │ GPT-4.1  │ │ Claude 4 │ │   V3     │ │  GLM-4   │ │ Gemini   │  │   │
│  │  └──────────┘ └──────────┘ └──────────┘ └──────────┘ └──────────┘  │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                    │                                        │
│                                    ▼                                        │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                       Router (智能路由)                              │   │
│  │  ┌─────────────────────────────────────────────────────────────┐   │   │
│  │  │  能力匹配 → 资源可用 → 成本优化 → 质量保证 → 熔断降级         │   │   │
│  │  └─────────────────────────────────────────────────────────────┘   │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                    │                                        │
│                                    ▼                                        │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                       Local Providers (本地)                         │   │
│  │  ┌──────────────────────────────────────────────────────────────┐  │   │
│  │  │  M4 Max Ollama (localhost:11434)                             │  │   │
│  │  │  ├── CodeGeeX4-9B (代码生成)                                 │  │   │
│  │  │  ├── Qwen2.5-7B (通用对话)                                   │  │   │
│  │  │  ├── Nomic-Embed (文本嵌入)                                  │  │   │
│  │  │  └── DeepSeek-V3-Cloud (云端代理)                            │  │   │
│  │  └──────────────────────────────────────────────────────────────┘  │   │
│  │  ┌──────────────────────────────────────────────────────────────┐  │   │
│  │  │  iMac M4 Ollama (待部署)                                     │  │   │
│  │  │  ├── ChatGLM3-6B (中文对话)                                  │  │   │
│  │  │  └── Phi-3-mini (快速响应)                                   │  │   │
│  │  └──────────────────────────────────────────────────────────────┘  │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                    │                                        │
│                                    ▼                                        │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                       Agent Pool (7大智能体)                         │   │
│  │  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐       │   │
│  │  │Navigator│ │ Thinker │ │ Prophet │ │  Bole   │ │Sentinel │       │   │
│  │  └─────────┘ └─────────┘ └─────────┘ └─────────┘ └─────────┘       │   │
│  │  ┌─────────┐ ┌─────────┐                                           │   │
│  │  │  Pivot  │ │Grandmstr│                                           │   │
│  │  └─────────┘ └─────────┘                                           │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 7.2 Provider 配置

#### API Providers (云端)

| Provider | 模型 | 用途 | 成本 | 优先级 |
|----------|------|------|------|--------|
| OpenAI | GPT-4.1, GPT-4o | 通用对话 | 高 | P2 |
| Anthropic | Claude 4 | 深度推理 | 极高 | P1 |
| DeepSeek | V3, R1 | 代码+推理 | 中 | P3 |
| 智谱 | GLM-4, ChatGLM3-6B | 中文优化 | 低 | P4 |
| Google | Gemini 2.5 Pro | 多模态 | 中 | P3 |
| Groq | Llama, Mixtral | 高速推理 | 低 | P4 |

#### Local Providers (本地)

| Provider | 模型 | 部署位置 | 推理引擎 | 优先级 |
|----------|------|----------|----------|--------|
| Ollama (M4 Max) | CodeGeeX4-9B, Qwen2.5-7B, Nomic-Embed | M4 Max | Ollama | P1 |
| Ollama (iMac M4) | ChatGLM3-6B, Phi-3-mini (待部署) | iMac M4 | Ollama | P2 |
| HuggingFace | CogVideoX-5B, CogAgent-9B | M4 Max | Transformers | P3 |

### 7.3 智能路由策略

#### 路由规则

```typescript
interface RouterConfig {
  // 能力匹配
  capabilityMapping: {
    code_generation: ['codegeex4:latest', 'deepseek-v3'],
    deep_reasoning: ['deepseek-v3', 'claude-4'],
    quick_response: ['phi3:mini', 'qwen2.5:7b'],
    chinese_chat: ['chatglm3:6b', 'qwen2.5:7b'],
    embedding: ['nomic-embed-text'],
  };
  
  // 资源阈值
  resourceThresholds: {
    memory: 0.8,  // 内存使用率阈值
    gpu: 0.9,     // GPU 使用率阈值
    latency: 500, // 延迟阈值 (ms)
  };
  
  // 熔断配置
  circuitBreaker: {
    failureThreshold: 3,
    resetTimeout: 60000,
    halfOpenAttempts: 1,
  };
}
```

#### 路由算法

```typescript
function selectModel(task: AgentTask): RouterDecision {
  const { type, complexity, priority, accuracy } = task;
  
  // 1. 检查本地资源
  const localResources = checkLocalResources();
  
  // 2. 根据任务类型筛选候选模型
  const candidates = CAPABILITY_MAPPING[type] || [];
  
  // 3. 根据复杂度排序
  const sorted = sortByComplexity(candidates, complexity);
  
  // 4. 根据资源可用性选择
  if (localResources.memory < THRESHOLDS.memory) {
    // 本地资源紧张，使用 API
    return selectAPIModel(sorted);
  }
  
  // 5. 优先使用本地模型
  return selectLocalModel(sorted);
}
```

### 7.4 熔断与降级

#### 熔断策略

```yaml
circuit_breaker:
  failure_threshold: 3      # 连续失败阈值
  reset_timeout: 60000      # 熔断时间 (ms)
  half_open_attempts: 1     # 半开尝试次数
  recovery_threshold: 2     # 恢复阈值
```

#### 降级策略

```
API 失败 → 切换本地模型
本地失败 → 切换 API
主模型失败 → 切换备用模型
全部失败 → Mock 响应
```

---

## 8. 部署实施记录

### 8.1 已完成任务

| 任务 | 执行时间 | 状态 | 备注 |
|------|----------|------|------|
| M4 Max Ollama 安装 | 2026-02-17 | ✅ | 版本 0.5.x |
| CodeGeeX4-9B 部署 | 2026-02-22 | ✅ | Q4_0 量化 |
| Qwen2.5-7B 部署 | 2026-02-22 | ✅ | Q4_K_M 量化 |
| Nomic-Embed 部署 | 2026-02-17 | ✅ | F16 |
| CogVideoX-5B 下载 | 2026-02-22 | ✅ | HuggingFace |
| CogAgent-9B 下载 | 2026-02-22 | ✅ | HuggingFace |
| NAS Ollama 部署 | 2026-02-21 | ✅ | 无模型 |
| pgvector 部署 | 2026-02-22 | ✅ | NAS Docker |

### 8.2 待完成任务

| 任务 | 计划时间 | 优先级 | 备注 |
|------|----------|--------|------|
| ChatGLM3-6B 部署 (M4 Max) | Week 1 | P1 | Navigator 主模型 |
| Phi-3-mini 部署 (iMac M4) | Week 1 | P1 | Sentinel 主模型 |
| iMac M4 Ollama 安装 | Week 1 | P1 | 辅助节点 |
| Qwen2.5-14B 部署 (M4 Max) | Week 2 | P2 | Grandmaster 备用 |
| API Keys 配置 | Week 2 | P1 | DeepSeek, OpenAI 等 |
| 智能路由实现 | Week 3 | P2 | LLM Bridge |

### 8.3 部署命令记录

#### M4 Max 模型部署

```bash
# 已执行
ollama pull codegeex4:latest
ollama pull qwen2.5:7b
ollama pull qwen2.5-coder:1.5b
ollama pull nomic-embed-text

# 待执行
ollama pull chatglm3:6b
ollama pull qwen2.5:14b
ollama pull phi3:mini
```

#### iMac M4 模型部署 (待执行)

```bash
# 安装 Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 部署模型
ollama pull chatglm3:6b
ollama pull phi3:mini
ollama pull qwen2.5:7b
```

---

## 9. 监控与优化策略

### 9.1 性能监控

#### 监控指标

| 指标 | 目标值 | 告警阈值 |
|------|--------|----------|
| 推理延迟 | < 500ms | > 1000ms |
| 吞吐量 | > 30 tokens/s | < 15 tokens/s |
| 内存使用 | < 80% | > 90% |
| GPU 使用 | < 85% | > 95% |
| 错误率 | < 1% | > 5% |

#### 监控脚本

```bash
#!/bin/bash
# 文件: /Users/yanyu/YYC3-Mac-Max/Family-π³/scripts/monitor-models.sh

echo "=== YYC³ AI Family 模型监控 ==="
echo ""

# 检查 Ollama 服务
echo "📦 Ollama 服务状态:"
curl -s http://localhost:11434/api/tags | jq '.models[] | {name, size}'

echo ""
echo "💾 系统资源:"
echo "  内存: $(free -h | grep Mem | awk '{print $3 "/" $2}')"
echo "  GPU: $(nvidia-smi --query-gpu=utilization.gpu --format=csv,noheader 2>/dev/null || echo 'N/A')"

echo ""
echo "⚡ 推理测试:"
time curl -s http://localhost:11434/api/generate -d '{
  "model": "qwen2.5:7b",
  "prompt": "Hello",
  "stream": false
}' | jq '.total_duration / 1000000000 | "推理耗时: " + tostring + "s"'
```

### 9.2 优化策略

#### 模型量化优化

| 模型 | 当前量化 | 优化方案 | 预期收益 |
|------|----------|----------|----------|
| CodeGeeX4-9B | Q4_0 | Q4_K_M | 质量提升 5% |
| Qwen2.5-7B | Q4_K_M | Q5_K_M | 质量提升 3% |
| ChatGLM3-6B | - | Q4_K_M | 平衡质量/速度 |

#### 并发优化

```yaml
concurrency:
  m4_max:
    max_concurrent: 3
    queue_size: 10
    timeout: 30000
  iMac_m4:
    max_concurrent: 2
    queue_size: 5
    timeout: 30000
```

#### 缓存优化

```yaml
cache:
  embedding:
    enabled: true
    ttl: 86400
    max_size: 10000
  response:
    enabled: true
    ttl: 3600
    max_size: 1000
```

---

## 附录

### A. 模型下载命令速查

```bash
# M4 Max
ollama pull codegeex4:latest      # ✅ 已安装
ollama pull qwen2.5:7b            # ✅ 已安装
ollama pull nomic-embed-text      # ✅ 已安装
ollama pull chatglm3:6b           # 待安装
ollama pull phi3:mini             # 待安装
ollama pull qwen2.5:14b           # 待安装

# iMac M4 (待执行)
ollama pull chatglm3:6b
ollama pull phi3:mini
ollama pull qwen2.5:7b
```

### B. API 配置参考

```bash
# .env.api
DEEPSEEK_API_KEY=sk-xxx
DEEPSEEK_BASE_URL=https://api.deepseek.com/v1

OPENAI_API_KEY=sk-xxx
OPENAI_BASE_URL=https://api.openai.com/v1

ANTHROPIC_API_KEY=sk-xxx
ANTHROPIC_BASE_URL=https://api.anthropic.com

ZHIPU_API_KEY=xxx
ZHIPU_BASE_URL=https://open.bigmodel.cn/api/paas/v4
```

### C. 验证命令

```bash
# 检查模型列表
curl http://localhost:11434/api/tags | jq '.models[].name'

# 测试推理
curl http://localhost:11434/api/generate -d '{
  "model": "qwen2.5:7b",
  "prompt": "你好",
  "stream": false
}'

# 检查嵌入
curl http://localhost:11434/api/embeddings -d '{
  "model": "nomic-embed-text",
  "prompt": "测试文本"
}'
```

---

<div align="center">

**YYC³ AI Family**

*言启象限 | 语枢未来*

**万象归元于云枢 | 深栈智启新纪元**

**亦师亦友亦伯乐；一言一语一协同**

---

*文档版本: 1.1.0*
*最后更新: 2026-02-22*
*作者: YYC³ Team*

</div>
